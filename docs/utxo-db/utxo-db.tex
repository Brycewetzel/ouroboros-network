% -*- fill-column: 80 -*-

\documentclass[11pt,a4paper]{article}

\usepackage[margin=2.5cm]{geometry}
\usepackage[dvipsnames]{xcolor}
\usepackage{todonotes}
\usepackage{microtype}
\usepackage{amssymb,amsmath}
\usepackage{mathpazo}
\usepackage{longtable,booktabs}
\usepackage{dcolumn}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage[capitalise,noabbrev,nameinlink]{cleveref}
\hypersetup{
  pdftitle={Storing the Cardano ledger state on disk: analysis and design options},
  pdfborder={0 0 0},
  breaklinks=true
}

\begin{document}

\title{Storing the Cardano ledger state on disk: \\
       analysis and design options \\
       {\large \sc An IOHK technical report}
  }
\date{Version 0.9, April 2021}
\author{Douglas Wilson     \\ {\small \texttt{douglas@well-typed.com}} \\
   \and Duncan Coutts      \\ {\small \texttt{duncan@well-typed.com}} \\
                              {\small \texttt{duncan.coutts@iohk.io}}
   }

\maketitle

%\listoftodos

\section{Introduction}
\label{introduction}

The project is intended to solve the following problem: the Cardano node keeps
its ledger state within memory (RAM) and as Cardano scales up this will not be
sustainable because the ledger state will eventually grow too big.

The solution that this project is focused on is to move the bulk of the ledger
state from memory to disk. This will involved developing and then integrating
new infrastructure in the Cardano node (ledger and consensus layers) to allow
large parts of the ledger state to be kept on disk rather than in memory.

This document is intended to capture the business and technical requirements,
to present a small set of candidate solutions, and to evaluate and justify a
preferred solution among those candidates. That solution should be justified in
complexity by the captured requirements.

\tableofcontents

\section{Summary}
\label{summary}

Memory (RAM) is very fast but memory space is relatively small and expensive.
Disks -- even SSDs -- are relatively slow, but disk space is relatively
plentiful and cheap.

Thus the challenge when adapting a design from memory to disk is to maintain
adequate performance. For blockchains generally, and Cardano specifically, this
is not a trivial problem. For example, for a long time Ethereum node performance
was bottlenecked on disk I/O performance. We must consider performance in the
design analysis or we will face the same problem.

Best case estimates indicate that the stretch goal of 200 TPS is \emph{very}
hard to achieve if one also wants the time to synchronise the chain to be
reasonable (e.g. the first time Daedalus is used). Even threshold or mid
targets of 20 and 50 TPS will be a challenge, while keeping sync times
reasonable.

The reason for this is clear: if we were to aim for syncing being 1000 times
faster than real time, then a year's worth of chain data would take just under
9 hours to validate. End users would have to wait 9 hours for each year that
the chain had been operating at this rate. Arguably, even this is unreasonably
slow, and yet note that this already requires syncing to be 1000 times faster.
So if our target were 200 TPS, then the requirement for syncing would be
200,000 TPS. It is intuitively clear that 20,000 or 200,000 TPS is a hard
target indeed.

Overall this points to the next major scaling bottleneck being synchronisation
performance. Though it is out of scope for this project, it will likely be
worth developing solutions that do not require all nodes (e.g. Daedalus nodes)
to download and validate the entire chain.

\subsection{Example estimate}
The simplified optimistic estimate is as follows:
\begin{itemize}
\item Assume the 200 TPS stretch goal.
\item Assume the typical 2 inputs and 2 outputs per tx.
\item Assume the UTxO is the only part of the ledger state of interest. This is
      a simplifying assumption. In reality there are other parts of the ledger
      state that will make these estimates worse.
\item Assume the UTxO mostly does not fit in memory.
\item Assume the UTxO read access pattern has poor temporal and physical
      locality, and thus each lookup will typically require at least one
      ``random'' disk read.
\item Assume that writes to the database are able to be efficiently dispatched
        in batches such that they are insignificant in cost relative to reads.
        Note that this is a rosy assumption that may be mostly true for LSM trees,
        but would not be true for many other data structures (e.g. B+ trees)
\item Assume a DB read amplification factor of 1.5. (This is a rosy assumption.)
\item Thus 200 TPS translates to $200 \times 2 \times 1.5 = 600$ disk I/O operations per
      second (IOPS).
\item Assume a mid-range SSD rated at 10,000 IOPS for random reads at queue
      depth 1, and 100,000 IOPS for random reads at queue depth 32.
\item Assume that we can fully utilise the parallel performance of the SSD,
      i.e. use queue depth 32.
\item The ratio of SSD performance IOPS (100,000) to live system IOPS (600)
      gives the sync speed ratio, i.e. the factor that syncing the chain would
      be compared to real time. This is a factor of $100,000 / 600 = 167$ in
      our example.
\item Thus for a chain growing at 200 TPS for a year, the time to sync that
      chain would be $1/167$ of a year, which is 52.5 hours, more than two days.
\end{itemize}

\subsection{Crucial factors}
As we can see in the estimate above, the chain sync times are not reasonable.
The example illustrates that the crucial factors are:
\begin{enumerate}
\item The target TPS
\item The read amplification factor
\item a database that can efficiently batch writes
\item The SSD random read performance
\end{enumerate}
Thus if we reduce the target TPS by a factor of 10, we reduce the sync time bound
correspondingly. With 20 rather than 200 TPS we could expect at best 5 hours of syncing
to catch up a year of the chain.

A read amplification factor of only 1.5 is itself a challenge, and requires a
good database choice.

Modern SSDs IOPS for random read range from 100,000 to 1,000,000 for the
extreme end of the consumer market. Achieving these levels of performance
requires utilising parallel I/O. If only serial I/O is used, one is limited to
the approximately 10,000 -- 20,000 IOPS. Using parallel I/O requires a more
complex design and developing other additional software infrastructure.

So as we can see, even with 20 TPS, to achieve reasonable sync times will
require a sophisticated choice of database, and the development effort needed
to take advantage of parallel I/O. Or it requires relaxing the assumption that
most of the ledger state does not fit in memory: allowing users with lots of
memory to sync quickly, while other users sync slowly.

\subsection{Recommendation}
The recommended development path is to assume that initially (e.g. 12 months)
the TPS will remain relatively low, and that the size of the ledger state will
remain only somewhat bigger than memory. In this case it may be possible to
develop a solution that does not initially use a very sophisticated database
and does not use parallel I/O, but follows a design that can be extended to do
so.

\section{Requirements and targets}
\label{requirements}

\subsection{Transaction Rate (TPS)}

The transaction rate is the average number of \emph{transactions per second}
(TPS). It is a measure of the rate that data is added to a blockchain and is
often used to compare different systems. It is a crude measure because it does
not take account of the transaction size, which can vary wildly. Nevertheless,
because it is the measure used to compare systems, our targets are also
expressed in TPS.

Our targets are

\begin{center}
\begin{tabular}[]{lr}
  Target    & TPS \\
  \toprule
  Threshold &  20 \\
  Middle    &  50 \\
  Stretch   & 200
\end{tabular}
\end{center}
%
For comparison, these are the current transaction rates of Cardano and
comparable systems%
\footnote{Bitcoin TPS charts \url{https://blockchair.com/bitcoin/charts/transactions-per-second}\\
Ethereum TPS charts \url{https://blockchair.com/ethereum/charts/transactions-per-second}}.

\begin{center}
\begin{tabular}[]{lr}
  System    & TPS (approx) \\
  \toprule
  Bitcoin                  & 4   \\
  Ethereum, peak           & 17  \\
  Cardano, mainnet typical & 1   \\
  Cardano, mainnet max     & 7   \\
  Cardano, benchmarks max  & \textgreater 50   \\
\end{tabular}
\end{center}

It is out of scope for this storage project to demonstrate the node running at
50 or 200 TPS. It is within scope to demonstrate that the storage system would
not be a bottleneck that would prevent the node running at 50 or 200 TPS.

\subsection{Ledger state size}
\label{ledger-state-size}

Cardano currently has approximately 2 million UTxO entries. For comparison,
Bitcoin currently has approximately 75 million UTxO entries%
\footnote{Bitcoin UTxO size charts \url{https://www.blockchain.com/charts/utxo-count}}.

Cardano also has delegation. The current ratio between UTxO entries
and registered stake keys is 5:1. We will assume this 5:1 ratio persists.

Our targets for UTxO sizes are

\begin{center}
\begin{tabular}[]{lrr}
  Target (millions) & UTxO entries & Stake keys \\
  \toprule
  Threshold &  10  &  2 \\
  Middle    &  50  & 10 \\
  Stretch   & 100  & 20
\end{tabular}
\end{center}

We assume the number of registered stake pools will remain in the low thousands.

\subsection{Resource Requirements}
\label{resource-requirements}

The resource requirements of the Cardano node need to remain reasonable, to
allow nodes runing on end user systems with Daedalus, and to keep hosting costs
for SPOs reasonable. The purpose of this project is to limit the growth in the
memory (RAM) requirement, while allowing for a substantial increase in size of
the ledger state generally by keeping most of it on disk. So the memory
requirement should remain steady or decrease while the disk requirements must
increase.

At the time of writing, the published requirements for the Cardano node%
\footnote{\url{https://github.com/input-output-hk/cardano-node/releases/tag/1.26.2}}
and for Daedalus%
\footnote{\url{https://iohk.zendesk.com/hc/en-us/articles/360010496553-Daedalus-System-Requirements}}
are:

\begin{center}
\begin{tabular}[]{lp{6cm}p{5cm}}
  Resource    & Node & Node + Daedalus \\
  \toprule
  CPU         &  x86 processor, 2+ cores at 1.6GHz (2GHz for a stake pool or relay)  & 64-bit dual core processor \\
  Memory      &  8 GB  & 8 GB \\
  Disk space  & 10 GB, 20 GB for a stake pool & 15 GB
\end{tabular}
\end{center}

These published requirements are not completely coherent, since the Daedalus
case includes the node, wallet backend and the Daedalus frontend, all of which
take resources.

No change in CPU requirement is expected. The memory requirement is expected
to remain steady or decrease:
\begin{center}
\begin{tabular}[]{lr}
  Memory target & GB \\
  \toprule
  Threshold &  8 \\
  Middle    &  4 \\
  Stretch   &  2
\end{tabular}
\end{center}
The crucial disk requirement will be disk random read performance (IOPS) not
disk space. The proposed requirements are
\begin{center}
\begin{tabular}[]{lr}
  Random 4k reads & IOPS \\
  \toprule
  Queue depth 1  &  10,000 \\
  Queue depth 32 & 100,000 \\
\end{tabular}
\end{center}
This implies a requirement for an SSD. Spinning disks cannot achieve these IOPS
requirements.

There are also disk space requirements for storing the ledger state on an SSD.
Given the target of 10--100 million UTxO entries, and corresponding stake keys,
and three stake distribution snapshots, the expected disk requirement for the
ledger state are
\begin{center}
\begin{tabular}[]{lrr}
  Target  & UTxO entries (millions) & SSD disk space (GB) \\
  \toprule
  Threshold &  10  &  20 \\
  Middle    &  50  & 100 \\
  Stretch   & 100  & 200
\end{tabular}
\end{center}
In addition, disk space is needed to store the block chain itself. This does
not require an SSD and can use cheaper spinning magnetic disk. The disk space
needed grows continuously, at a rate that depends on the TPS.
\begin{center}
\begin{tabular}[]{lrrr}
                      & TPS & disk space per day (MB) & per year (GB) \\
  \toprule
  Mainnet typical     &   1 &    39 &    14 \\
  Mainnet current max &   7 &   270 &    96 \\
  Threshold target    &  20 &   770 &   275 \\
  Middle target       &  50 & 1,929 &   687 \\
  Stretch target      & 200 & 7,714 & 2,750
\end{tabular}
\end{center}
It is worth noting that the upper end is completely impractical for a public
system where all partipants download and retain the entire chain as it would
require everyone running a node to buy extra multi-terabyte hard drives per
year.

\subsection{Performance Requirements}
\label{performance-requirements}

As described in the summary, we expect the time to synchronise the whole chain
to be the hardest performance requirement to achieve. This is because it
involves doing the same thing as the node does to validate the chain normally,
but 100s of times faster, so that the node can sync in a reasonable time frame.

We have no specific requirements for sync time, but reasonable values would be
\begin{center}
\begin{tabular}[]{lrr}
  Target    & faster than real time & hours to sync 1 year \\
  \toprule
  Threshold &  100$\times$ &  88 hours \\
  Stretch   & 1000$\times$ & 8.8 hours
\end{tabular}
\end{center}


\subsection{Functional Requirements}

The storage system must support the needs of the consensus and ledger for
storing the bulk of the ledger state on disk. It must support all the operations
that these components need to interact with the selected parts of the ledger
state that are kept on disk.

The ledger state consists of small complicated parts and large simple parts.
Analysis indicates that to meet the memory requirements, only the large simple
parts of the ledger state need to be kept on disk. It is acceptable to keep the
other parts of the ledger state in memory.

The large parts of the ledger state that must be kept on disk are:
\begin{description}
\item[UTxO]
    The collection of all unspent transaction outputs.
\item[Stake addresses \& delegation]
    The collection of all registered stake addresses, their corresponding
    reward account balances, and their choice of delegation to a stake pool.
\item[Stake distribution]
    An aggregation of the UTxO by stake address, recording for each state
    address the amount of stake controlled by that address.

    Three snapshots of this stake distribution are required.
\end{description}
The storage system must support all the operations the consensus and ledger
layers need for interacting with these parts of the ledger state.

For an incremental delivery of a solution, it would be acceptable to start with
the UTxO and then proceede to the two stake collections. The UTxO is the largest
of the collections, but the two stake collections are also of a substantial
size. To achieve the ledger size requirements from \cref{ledger-state-size}
within the memory requirements from \cref{resource-requirements} will require
all three large parts of the ledger state to be kept on disk.

\section{Related components}
\label{components}

The related components are from the consensus and ledger layers.

\subsection{Consensus}

The components that any new code would have to integrate with directly are from
the consensus layer. The consensus layer already has three subcomponents that
manage on-disk data structures as part of its storage subsystem, for storing:
\begin{itemize}
\item the old immutable part of the block chain
\item the recent volatile part of the block chain
\item snapshots of the ledger state
\end{itemize}
Storing parts of the ``live'' ledger state would add a fourth such subcomponent.

The consensus layer performs all the state management on behalf of the ledger
layer. Changes will be required in this state management to accomodate keeping
parts of the ledger state on disk.

The design of the interface between the main consensus code and the extra
storage subcomponent will be crucial for the success of the project: both for
the effort and disruption to the consensus code and also the performance
possiblities or limitations.

The (extensive) automated consensus tests rely on being able to mock out the
underlying file system used by the storage components, replacing it with a
simulted file system. The simulted file system supports deliberate scripted
fault injection. The tests rely on this to be able to test that the storage
components and the consensus layer as a whole is fully robust in the face of
I/O failures and disk corruption. The existing storage components are written
in terms of a file system interface, which allows both the real file system for
production, and a mock file system instance for the tests.

To maintain the ability to test that integration of the whole consensus layer
is robust to file system failure and corruption, will require that new storage
components are also written against the file system interface, rather than using
the file system directly.

\subsection{Ledger}

Although the ledger layer does not interact with disk storage directly, the
design of the storage system is heavily influenced by the needs and structure
of the ledger layer. The consensus and ledger components share an interface,
enabling the consensus layer to perform all the state management on behalf of
the ledger. Currently this interface relies on the full ledger state being kept
in memory. This interface will have to be adjusted to enable the consensus
layer to be able to keep the large parts of the ledger state on disk. This
interface change will obviously have an impact on the ledger code.

The design of this interface will also be crucial to success of the project:
both for the effort and disruption to the ledger code and also the performance
opportunities or limitations.

\subsection{Cardano DB-Sync \& Cardano API clients}

The Cardano node provides an interface for other client applications to access
the blockchain data. Some client applications also need access to the ledger
state as they process the blockchain. In particular the DB-Sync component
makes use of this. More generally the Cardano API is in the process of being
extended to make it easier to write client applications that make use of the
ledger state. Currently these client applications keep their own copy of the
ledger state in memory.

Eventually it will be necessary for these client applications, and DB-Sync in
particular, to also transition to keeping their copies of the ledger state on
disk. The need for this transition is less urgent for applications like DB-Sync
than it is for the node itself, because these applications are typically hosted
only on servers with more plentiful resources, whereas the node is intended to
be deployed in more locations where the resource limits are tighter.

It would save development effort overall if client applications can reuse the
ledger state storage components developed for the node.

\section{Technical constraints}
\label{constraints}

\subsection{UTxO access pattern}
\label{workload}

The access pattern for the UTxO data structure is \emph{very} write heavy. In
the usual case, we expect exactly three operations for each UTxO entry over its
lifetime:
\begin{enumerate}
\item One {\sc insert} operation when the UTxO entry is created.
\item One {\sc lookup} operation when validating the block in which the UTxO
      entry is spent.
\item One {\sc delete} operation when the block that spends the UTxO entry is
      added to the chain.
\end{enumerate}
This means that processing a typical transaction with 2 inputs and 2 outputs
will entail:
\begin{itemize}
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{1pt}%
\item Two {\sc lookup} operations: one for each input.
\item Two {\sc delete} operations: one for each input.
\item Two {\sc insert} operations: one for each output.
\end{itemize}
This gives us a 1:2 read:write ratio; for each {\sc lookup} there is one
{\sc insert} and {\sc delete}. This is very write heavy, and is very different
from typical database applications that are read heavy (e.g. a 10:1 read:write
ratio). Mainstream database designs are optimised for read heavy workloads.
There are totally different database designs optimised for write heavy
workloads.

Note that some UTxOs will be read more often -- if validation of a block fails
-- or if a block is validated multiple times. We expect the vast majority of
UTxOs to be read exactly once, and this applies moreso while syncing, where we
do not expect validation to fail. By definition, UTxOs will be written at most
twice: once to create and once to delete.

Given the targets for TPS in \cref{requirements-tps} and sync speed in
\cref{performance-requirements}, it is useful to see these means as a
requirement for the storage system. It is illustrative to look at the effective
TPS that would be needed to be able to sync 100 or 1000 times faster than real
time, for different TPS targets. The table below also shows the corresponding
number of {\sc lookup} and {\sc insert}/{\sc delete} operations per second,
assuming the typical transaction of 2 inputs and 2 outputs.
\begin{center}
\begin{tabular}[]{rrrrr}
  Target TPS & target sync factor
             & effective TPS
             & {\sc lookup} ops/s
             & {\sc insert}/{\sc delete} ops/s \\
  \toprule
   20 TPS &   100$\times$ &   2,000 TPS &   4,000 &   8,000 \\
   20 TPS &  1000$\times$ &  20,000 TPS &  40,000 &  80,000 \\
  200 TPS &  1000$\times$ & 200,000 TPS & 400,000 & 800,000
\end{tabular}
\end{center}

\subsection{integrate with io-sim}
\label{io-sim}
The Ledger and Consensus codebases have achieved impressive quality
and reliability, in no small part due to their use of the io-sim
Haskell library to test against a broad array of failure modes. This
relies upon integrating at the Haskell source level.

The solution should similarly demonstrate it's correctness with an io-sim test
suite. This is not a strict requirement, however it is difficult to see how one
could be confident in the correctness of the solution without it.

\subsection{minimize changes to STS}

The Cardano Ledger rules are implemented are formulated as executable
specifications (STS). \todo{right? citation?}. While the Byron ledger rules have
a separate implementation that is verified against the executable specification,
Shelley (and the following era's) use the executable specification directly.
This was a pragmatic choice at the time it was made, however it does introduce
tensions. The Ledger team are limited in their ability to modifying the
implementation of those rules to meet business requirements, because changing
the executable specification risks introducing unsoundness to the ledger rules.

The solution should minimise disruption to the implementation of the ledger
rules. This may require us to separate the implementation of the Shelley (and
forward) ledger rules from their executable specification.

\subsection{Batching IO}
\label{Batching IO}
Modern SSDs achieve IO-ops/S in the hundreds of thousands operating concurrently.
Typically they will server up to 32 operations at the same time. For software to
saturate the bandwidth of the disk, that software must be prepared to issue many
operations simultaneously. There is a trade-off here between performance and
complexity. To saturate the bandwidth of the disk, as we expect to be necessary
to achieve reasonable sync times \todo{add ref}, will require more complexity in
the solution.

It is very likely that a solution required to perform well with a growing
blockchain will need to exploit Batching IO to achieve this.

\subsection{Operations}
\label{Operations}
Here we describe the operations the Ledger state on-disk data store should
support.

\begin{enumerate}
  \item INSERT
        The data store must be able to store data against a key. There will be
        some flexibility in how updates to data are applied. Stake delegation
        and aggregation may benefit from some more specialised operations,
        though likely a simple INSERT with the new value would suffice.
  \item LOOKUP
        The data store must be able to retrieve the data previously stored
        against a key
  \item DELETE
        The data store must be able to delete a key and the data stored against
        it. This is important for storing UTxOs, because the growth of unspent
        transaction outputs grows much more slowly than the total historical
        transaction outputs, and so deleting spent transactions will lighten the
        load on the data store manyfold.
  \item ROLLBACK
        \label{ROLLBACK}
        The consensus layer will rollback ledger state whenever it switches
        chain forks. The data store must support this in some way.
  \item

\end{enumerate}

\subsubsection{Locality}
\label{Locality}
Many data structures exploit locality in their data. For example, a time series database will store together values that are adjacent in time. In this way, queries which inspect values that are close in time (most all queries to a time series database!) are able to read a single disk page, and obtain many values of interest.

There are two obvious dimensions in locality for UTxOs: Time of creation and Id (A hash).

We can expect no locality in the ID. IDs are effectively random, and there is no reason to think that two UTxOs with similar IDs would be accessed together.

Less obviously, indeed empirically, there is little temporal locality in blockchains \todo{cite}.
Thus we have a pessimistic towards applying a caching to the Ledger State data structure. Nevertheless, see \ref{Caching Layer}

\section{Design options}
\label{options}

It's clear we will need to implement an disk Key-Value Store capable of serving
the ledger state. The natural choice in the literature for this problem is the Log Structured Merge Tree \todo{cite LSM}. \todo{describe LSM?, liberate a diagram from somewhere?}

\subsection{Off-the-shelf Key-Value Store}

\todo[inline]{
High chance of ``mostly working''
Unlikely to be as reliable as the rest of cardano node
Will likely increase deployment complexity (logging/monitoring/backups/etc)
Good for benchmarking
Add that Cardano used to use RocksDB, and it was problematic
}

One might ask why not use an existing on-disk Key-Value store, such as
RocksDB \todo{cite RocksDB}, or \todo{cite sqlite}? There are two
reasons that we might expect this to be unsatisfactory:

\subsubsection{io-sim testing}
The tooling described \ref{io-sim} would be difficult or impossible to use with
an Off-the-shelf Key-Value Store.

\subsection{Existing Haskell Libraries}

There are no mature LSM libraries on hackage. There does exist a B+-tree
implementation \todo{cite https://hackage.haskell.org/package/haskey-btree},
which we could likely turn to our needs.

A B+-tree will not scale as far as a LSM, but for existing transaction rates it
may suffice.

Implementing the On-disk Ledger State with this library would allow us to ship a
solution with significantly less effort than writing an On-Disk data structure
from scratch. We would also expect less risk of delivery failure. We would be
able to leverage the io-sim library to give confidence of quality.

Note however that a solution based on B+-trees is unlikely to satisfy the
requirements \ref{performance-requirements}.

\subsection{Bespoke LSM Tree}

Since no sufficient on disk data structure exists, we could create one. A data
structure in the literature, the LSM Tree \cite{monkey}, is appropriate. It offers:

\begin{enumerate}
  \item Append only writes to disk
        This is critical for good write performance. There no need to ``sync''
        with the disk, meaning the program never needs to wait for the disk to be ready.
  \item A snapshotting mechanism
        In normal operation the Cardano Node frequently needs to roll back the
        ledger state, see \ref{ROLLBACK}.
  \item Workload
        There is a  well specified recipe in \cite{monkey} for tuning  the data
        structure to various workloads. Our workload \ref{workload} is quite unusual. We
        would be able to tune the LSM tree to match that workload, and moreover, if the
        workload changes in the future as Cardano evolves, we will have the opportunity
        to revisit these decisions without reworking the entire data structure.
\end{enumerate}

We would plan to follow the blueprint laid out by \cite{monkey}. This approach
does carry a higher risk of failure, as a greenfield project with many unknowns.

\subsection{Batched IO}
As described in \ref{Batching IO}, we will have the option to use Batching IO,
at the cost of additional complexity in the implementation. While a scalable
solution will likely need to do this, it is likely wise to start with a simpler,
slower IO model, and then add Batching IO once we have demonstrated the
correctness of that simpler implementation.

\subsection{Interface Style}

The decision for which on disk data structure to use, and how to implement it,
is largely orthogonal to the decisions around what the interface to that data
structure should look like.

A naive ``CRUD'' interface to the data structure would be simpler to work with,
and would perhaps ease integration of the data structure into the Related Components
codebases. However this would impede our exploitation of Batched IO. \ref{Batching IO}.

Instead, we recommend an interface which allows the data structure to read/write
many values in parallel. Note that committing to this interface does not require
us to implementing parallel operations immediately. A parallel interface could
be implemented using an internal ``CRUD'' interface.

e.g.
readUTxOs :: [TxIn] -> (Map TxIn TxOut -> m a) -> m a
writeUTxOs :: [(TxIn, Maybe TxOut)] -> m ()

\todo{code? delete or format properly}

Note: We intend to elaborate on this interface, and propose a design, in a
following document.

\subsection{Caching Layer}
\label{Caching Layer}

Although we expect caching to perform poorly over this data structure, due to the
lack of locality \ref{Locality}, perhaps a poor cache would be significantly
better than nothing.

One option to mitigate long sync times may be to allow users to add a large
in-memory cache over the data structure while syncing. In this way, users could
sync on a large cloud machine (Say 64GB RAM), save their state to disk, then run
their node on a smaller cloud machine using that synced state.


\subsection{Hybrid options}

We are not bound to a single choice of those above. Although we will recommend that
a bespoke LSM tree should be the long term goal, it may be the case that delivery of a
working solution, that does not meet the performance requirements as such, is
the best first step.

We should aim to specify an interface with can be served by any of the three
options above.

\section{Preferred option}
\label{preferred}

Our preferred option is a hybrid, seeking to fail early, minimize delivery risk,
deliver some improvement early, and giving the option of pausing work before
completion. We will design the API early and implement several iterations of the
data structure, each with more complexity and better performance than the
previous.

Firstly, we would deliver a Draft API for agreement with stakeholders (Ledger
and Consensus). This would be prioritised immediately. Once agreed, we would
deliver a trivial implementation of a data store that was still entirely in
memory, using the existing algorithms, but which is interacted with through the
newly agreed API.

Ledger and Consensus will then be unblocked from beginning to integrate, i.e.
modifying their code to use the new API.

In parallel with this, we will begin implementing a B+-tree. The focus will
be to produce a working solution, not to achieve good performance. We
will verify the correctness of this implementation with io-sim. The intention
will be to use this as a skeleton with with to implement an LSM tree.

Once the B+-tree is feature complete, and Related Components have completed
their integration, we would have the option to tune the performance of the
B+-tree so that it could be deployed into production. Whether we would do this
or not would depend on the urgency of lowering the memory footprint of Cardano
Node.

We will then replace the B+-tree in the previous solution with a bespoke LSM tree.
Our implementation will be written to use batched IO from the outset. We will
verify it's correctness with the test suite developed against the B+tree
implementation. We will iterate on this LSM tree, improving it's performance
as required.

\section{Risks}

Here we call out specific risks we see in the project, and in our preferred option in particular
likely

\subsection{API design}
The design of the API will have far reaching impact on the success of the
project. We need to carefully balance the need to integrate smoothly with Ledger
and Consensus and the requirement that the API can be implemented with Parallel
IO. This points one towards fixing the API early, so that integration in the
related components can start, however this risks increasing the costs, or
rendering impossible, the incorporation of learnings from our experience
implementing the data structure into the design of the final API.

\subsection{Performance of Chain Sync}
As discussed in \todo{ref section} there is reason to expect that any solution
will have sync times in the order of days per chain-year at even moderate
Transaction rates. This is the critical constraint that guides the majority of
our design. There is some opportunity to mitigate this e.g. \ref{Caching Layer},
but this will remain a looming threat if transaction rates increase
substantially. We recommend that IOHK direct it's researchers to investigate
ways that nodes can sync a chain without having to validate the entire history
of the chain.

\addcontentsline{toc}{section}{References}
\bibliographystyle{plainnat}
\bibliography{utxo-db}

\end{document}
